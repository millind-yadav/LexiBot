# âš–ï¸ LexiBot - AI-Powered Legal Assistant

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.10%2B-blue)
![React](https://img.shields.io/badge/react-18%2B-cyan)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115%2B-green)
![Ollama](https://img.shields.io/badge/Ollama-Llama3-orange)

**LexiBot** is an advanced, local-first legal AI agent designed to assist with contract analysis, clause extraction, and risk assessment. Built with a privacy-centric architecture, it processes sensitive legal documents entirely within your local environment using **Retrieval-Augmented Generation (RAG)** and **Agentic Workflows**.

Unlike standard chatbots, LexiBot uses a **Planning Engine** to break down complex legal queries into executable steps, ensuring accurate and cited answers from your documents.

![LexiBot Interface](git assets/Screenshot 2025-12-05 at 15.23.47.png)

---

## ğŸš€ Key Features

- **ğŸ“„ Document Analysis:** Upload PDF or DOCX contracts. LexiBot parses them client-side for maximum privacy.
- **ğŸ” Semantic Clause Retrieval:** Find specific clauses (e.g., "Termination", "Indemnity") even if the exact keywords aren't used, thanks to vector search and synonym expansion.
- **ğŸ§  Stateful Conversations:** The agent remembers context from previous turns, allowing for natural follow-up questions (e.g., "Explain that clause further").
- **ğŸ¤– Agentic Reasoning:** The backend uses a planning engine to orchestrate tools:
    - *Clause Retrieval Tool* (Semantic Search)
    - *Contract QA Tool* (RAG-based answering)
    - *Structured Data Extractor* (Entity extraction)
- **ğŸ”’ Privacy-First:** Runs 100% locally using **Ollama** for the LLM and **FAISS** for vector storage. No data leaves your machine.
- **ğŸ’¬ Modern UI:** A responsive React interface with streaming responses, file management, and markdown support.

---

## ğŸ› ï¸ Tech Stack

### Backend
- **Framework:** FastAPI
- **LLM Orchestration:** LangChain
- **Vector Store:** FAISS (Facebook AI Similarity Search)
- **Embeddings:** HuggingFace (`all-MiniLM-L6-v2`)
- **LLM Runtime:** Ollama (Llama 3)
- **Agent Engine:** Custom `LegalPlanningEngine` & `AgentExecutor`

### Frontend
- **Framework:** React (Vite)
- **Styling:** Tailwind CSS
- **Icons:** Lucide React
- **Document Parsing:** `pdfjs-dist` (PDF), `mammoth` (DOCX)

---

## ğŸ—ï¸ Architecture

```mermaid
graph TD
    User[User] -->|Uploads Doc| Frontend[React Frontend]
    Frontend -->|Parses Text| ClientParser[Client-Side Parser]
    ClientParser -->|Raw Text| Frontend
    Frontend -->|Query + History + Context| Backend[FastAPI Backend]
    
    subgraph "Backend Agent"
        Backend -->|Request| Agent[Agent Executor]
        Agent -->|Plan| Planner[Planning Engine]
        Planner -->|Steps| Agent
        
        Agent -->|Execute| Tools[Tool Registry]
        
        subgraph "RAG Pipeline"
            Tools -->|Clause Retrieval| RAG[RAG Service]
            RAG -->|Embed| HF[HuggingFace Embeddings]
            RAG -->|Search| FAISS[FAISS Vector Store]
        end
        
        Tools -->|Generate Answer| Ollama[Ollama (Llama 3)]
    end
    
    Agent -->|Final Response| Backend
    Backend -->|JSON| Frontend
```

---

## âš¡ Getting Started

### Prerequisites
- **Python 3.10+**
- **Node.js 18+** (and `pnpm`)
- **Ollama** installed and running (`ollama serve`)

### 1. Setup Ollama
Ensure Ollama is running and pull the Llama 3 model:
```bash
ollama pull llama3
```

### 2. Backend Setup
Navigate to the project root:
```bash
# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
cd backend
pip install -r requirements.txt
# Install additional RAG dependencies
pip install langchain-community langchain-huggingface faiss-cpu

# Start the API server
python3 -m uvicorn app.main:app --reload --port 8002
```
*The backend will run on `http://localhost:8002`.*

### 3. Frontend Setup
Open a new terminal:
```bash
cd frontend

# Install dependencies
pnpm install

# Start the development server
pnpm dev
```
*The frontend will run on `http://localhost:5173`.*

### 4. Docker Setup (Optional)
If you prefer to run everything in containers:
```bash
# Build and start services
docker-compose up --build
```
*The app will be available at `http://localhost:3000`.*

---

## ğŸ“‚ Project Structure

```
LexiBot/
â”œâ”€â”€ agent/                  # Core agent logic and planning engine
â”‚   â””â”€â”€ core/
â”‚       â””â”€â”€ planning_engine.py
â”œâ”€â”€ backend/                # FastAPI application
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agent/          # Agent executor and prompt templates
â”‚   â”‚   â”œâ”€â”€ core/           # RAG service and config
â”‚   â”‚   â”œâ”€â”€ llm/            # Ollama client wrapper
â”‚   â”‚   â”œâ”€â”€ tools/          # Tools (Clause Retrieval, Contract QA)
â”‚   â”‚   â””â”€â”€ main.py         # API Entrypoint
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ frontend/               # React application
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/     # ChatInterface and UI components
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ’¡ Usage Guide

1.  **Open the App:** Go to `http://localhost:5173`.
2.  **Upload a Contract:** Click the paperclip icon and select a PDF or DOCX file.
3.  **Ask Questions:**
    -   *"Explain the termination conditions."*
    -   *"What are the tenant's liabilities?"*
    -   *"Are there any clauses regarding viewings?"*
4.  **Follow Up:**
    -   *"Explain that clause in simpler terms."*
    -   *"What are the risks associated with it?"*

---

## ğŸ”§ Configuration

- **Backend Port:** Defaults to `8002`. Configurable in `backend/app/main.py`.
- **LLM Model:** Defaults to `llama3`. Configurable in `backend/app/core/config.py` or via `OLLAMA_MODEL` env var.
- **RAG Settings:** Chunk size and overlap can be tuned in `backend/app/core/rag.py`.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License.
